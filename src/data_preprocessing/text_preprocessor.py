import re
import unicodedata
import pandas as pd
import logging
import os


logger = logging.getLogger(__name__)

# --- Amharic Character Mappings (from references) ---
# Map for common Amharic character variations to a canonical form
# This mapping needs to be comprehensive based on observed data.
# This is a representative sample based on typical Amharic preprocessing needs.
AMHARIC_CHAR_MAP = {"·àê":"·àÄ","·àë":"·àÅ","·àí":"·àÇ","·àì":"·àÉ","·àî":"·àÑ","·àï":"·àÖ","·àñ":"·àÜ",\
                       "·äÄ":"·àÄ","·äÅ":"·àÅ","·äÇ":"·àÇ","·äÉ":"·àÉ","·äÑ":"·àÑ","·äÖ":"·àÖ","·äÜ":"·àÜ",\
                       "·à†":"·à∞","·à°":"·à±","·à¢":"·à≤","·à£":"·à≥","·à§":"·à¥","·à•":"·àµ","·à¶":"·à∂","·àß":"·à∑",\
                       "·ãê":"·ä†","·ãë":"·ä°","·ãí":"·ä¢","·ãì":"·ä£","·ãî":"·ä§","·ãï":"·ä•","·ãñ":"·ä¶",\
                       "·å∏":"·çÄ","·åπ":"·çÅ","·å∫":"·çÇ","·åª":"·çÉ","·åº":"·çÑ","·åΩ":"·çÖ","·åæ":"·çÜ"}
# AMHARIC_CHAR_MAP = {
#     '·àÉ': '·àÄ', '·àÅ': '·àÅ', '·àÇ': '·àÇ', '·àÑ': '·àÑ', '·àÖ': '·àÖ', '·àÜ': '·àÜ', # ·àÄ variants
#     '·àè': '·àã', '·àâ': '·àâ', '·àä': '·àä', '·àå': '·àå', '·àç': '·àç', '·àé': '·àé', # ·àà variants
#     '·äã': '·àê', '·àó': '·àê', '·àë': '·àë', '·àí': '·àí', '·àî': '·àî', '·àï': '·àï', '·àñ': '·àñ', # ·àê variants
#     '·àü': '·àõ', '·àô': '·àô', '·àö': '·àö', '·àú': '·àú', '·àù': '·àù', '·àû': '·àû', # ·àò variants
#     '·àß': '·à†', '·à°': '·à†', '·à¢': '·à†', '·à§': '·à†', '·à•': '·à†', '·à¶': '·à†', # ·à† variants
#     '·àØ': '·à´', '·à©': '·à©', '·à™': '·à™', '·à¨': '·à¨', '·à≠': '·à≠', '·àÆ': '·àÆ', # ·à® variants
#     '·à∑': '·à≥', '·à±': '·à±', '·à≤': '·à≤', '·à¥': '·à¥', '·àµ': '·àµ', '·à∂': '·à∂', # ·à∞ variants
#     '·àø': '·àª', '·àπ': '·àπ', '·à∫': '·à∫', '·àº': '·àº', '·àΩ': '·àΩ', '·àæ': '·àæ', # ·à∏ variants
#     '·ã∑': '·ã≥', '·ã±': '·ã±', '·ã≤': '·ã≤', '·ã¥': '·ã¥', '·ãµ': '·ãµ', '·ã∂': '·ã∂', # ·ã∞ variants
#     '·åá': '·åÄ', '·åÅ': '·åÅ', '·åÇ': '·åÇ', '·åÑ': '·åÑ', '·åÖ': '·åÖ', '·åÜ': '·åÜ', # ·åÄ variants
#     '·åß': '·å£', '·å°': '·å°', '·å¢': '·å¢', '·å§': '·å§', '·å•': '·å•', '·å¶': '·å¶', # ·å† variants
#     '·çè': '·çã', '·çâ': '·çâ', '·çä': '·çä', '·çå': '·çå', '·çç': '·çç', '·çé': '·çé', # ·çà variants
#     '·äó': '·äì', '·äô': '·äô', '·äö': '·äö', '·äú': '·äú', '·äù': '·äù', '·äû': '·äû', # ·äò variants
#     '·äã': '·ãà', # Another form of ·äã
#     '·ãê': '·ä†', '·ãì': '·ä†', # Different forms of 'Ayn' to 'Alef'
#     '·çÖ': '·åΩ', '·çÄ': '·å∏', # Ethiopian Tsadi (·å∏) and Tsadi (·çÖ)
#     '·àè': '·àà', # Another form of ·àè
#     '·å®': '·å†', # Example: sometimes ·å© is used instead of ·å•
#     '·â∏': '·ä®',
#     '·âØ': '·â†', # Vee sound in some keyboards, normalize to Be
# }

# Amharic Numerals to Arabic Numerals mapping
AMHARIC_NUMERAL_MAP = {
    '·ç©': '1', '·ç™': '2', '·ç´': '3', '·ç¨': '4', '·ç≠': '5',
    '·çÆ': '6', '·çØ': '7', '·ç∞': '8', '·ç±': '9', # ·ç∞·çª represents 100 in Ethiopian system (one hundred)
    # Note: Amharic numbers are often written with combinations, e.g., ·ç≤ (10), ·ç≥ (20), ·ç¥ (30) ... ·çº (10,000)
    # For simplicity, we convert single digit Geez numerals. More complex conversion might be needed
    # if compound Geez numbers are common in your data.
    '·ç≤': '10', '·ç≥': '20', '·ç¥': '30', '·çµ': '40', '·ç∂': '50',
    '·ç∑': '60', '·ç∏': '70', '·çπ': '80', '·ç∫': '90', '·çª': '100', '·çº': '10000'
}


def apply_unicode_normalization(text: str) -> str:
    """Applies Unicode Normalization Form C (NFC)."""
    return unicodedata.normalize('NFC', text)

def replace_amharic_characters(text: str) -> str:
    """Replaces common Amharic character variations with a canonical form."""
    for amh_char, canonical_char in AMHARIC_CHAR_MAP.items():
        text = text.replace(amh_char, canonical_char)
    return text

def normalize_amharic_numerals(text: str) -> str:
    """Converts Amharic numerals to Arabic numerals."""
    for amh_num, arabic_num in AMHARIC_NUMERAL_MAP.items():
        text = text.replace(amh_num, arabic_num)
    return text

def normalize_punctuation(text: str) -> str:
    """
    Normalizes Amharic punctuation characters to their ASCII equivalents.
    Removes other non-essential symbols.
    """
    text = text.replace('·ç¢', '.')  # Ethiopian full stop
    text = text.replace('·ç£', ',')  # Ethiopian comma
    text = text.replace('·ç§', ';')  # Ethiopian semicolon
    text = text.replace('·çß', '?')  # Ethiopian question mark
    text = text.replace('·ç°', ':')  # Ethiopian colon
    text = text.replace('·ç¶', '-')  # Ethiopian dash/hyphen (used for e.g. price)

    # Replace multiple consecutive spaces with a single space
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def remove_urls_mentions_hashtags(text: str) -> str:
    """Removes URLs, Telegram mentions (@username), and hashtags (#tag)."""
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # URLs
    text = re.sub(r'@\w+', '', text) # Mentions
    text = re.sub(r'#\w+', '', text) # Hashtags
    return text

def remove_emojis_and_non_amharic_non_ascii(text: str) -> str:
    """
    Removes emojis and characters that are not Amharic script, basic ASCII (English letters, numbers,
    common punctuation), or whitespace.
    """
    # Define a regex pattern that matches:
    # 1. Amharic script characters (\u1200-\u137F)
    # 2. Basic Latin characters (a-zA-Z)
    # 3. Digits (0-9)
    # 4. Basic punctuation and symbols often needed (.,!?;:()[]{}'\"-/)
    # 5. Whitespace (\s)
    # Characters outside this set will be removed.
    pattern = re.compile(r'[^\u1200-\u137F\u0020-\u007E\s]+')
    text = pattern.sub('', text)
    return text

def remove_extra_whitespace(text: str) -> str:
    """Replaces multiple spaces with a single space and strips leading/trailing whitespace."""
    return re.sub(r'\s+', ' ', text).strip()

# --- Stop Word Removal (Placeholder) ---
# A comprehensive Amharic stop word list is crucial.
# You might need to build one or find a reliable source.
# For now, this is a placeholder.
AMHARIC_STOP_WORDS = set([
    # '·äê·ãç', '·ä•·äì', '·ã®', '·â†', '·àà', '·ä®', '·àã·ã≠', '·ãç·àµ·å•', '·ä•·äï·ã∞', '·àù', '·àõ·àà·âµ', '·ä†·àà', '·ä†·ã≠·ã∞·àà·àù'
    # Add common Amharic stop words here
    # Example: '·äê·ãç', '·ä•·äì', '·ã®', '·â†', '·àà', '·ä®', '·àã·ã≠', '·ãç·àµ·å•', '·ä•·äï·ã∞', '·àù', '·àõ·àà·âµ', '·ä†·àà', '·ä†·ã≠·ã∞·àà·àù'
    # For a real project, this list would be much larger and potentially loaded from a file.
])

def remove_amharic_stopwords(text: str) -> str:
    """
    Removes Amharic stop words from the text.
    Requires a predefined list of stop words.
    """
    if not AMHARIC_STOP_WORDS:
        logger.warning("Amharic stop words list is empty. Stop word removal will not be effective.")
        return text
    
    words = text.split()
    filtered_words = [word for word in words if word not in AMHARIC_STOP_WORDS]
    return " ".join(filtered_words)

# --- Main Preprocessing Function ---
def preprocess_amharic_text(text: str, remove_stopwords: bool = False) -> str:
    """
    Applies a series of robust preprocessing steps to a single Amharic text string.

    Args:
        text (str): The input Amharic text string.
        remove_stopwords (bool): Whether to remove common Amharic stop words.
                                 Set to True if AMHARIC_STOP_WORDS is populated.

    Returns:
        str: The preprocessed text string.
    """
    if not isinstance(text, str):
        return "" # Handle non-string inputs gracefully

    text = apply_unicode_normalization(text)
    text = replace_amharic_characters(text)
    text = normalize_amharic_numerals(text) # Apply this before removing non-ASCII if numbers are important
    text = remove_urls_mentions_hashtags(text)
    text = remove_emojis_and_non_amharic_non_ascii(text) # Removes unwanted characters
    text = normalize_punctuation(text) # Normalize specific punctuation then remove extra spaces
    text = remove_extra_whitespace(text)

    if remove_stopwords:
        text = remove_amharic_stopwords(text)
        text = remove_extra_whitespace(text) # Clean up spaces after stop word removal

    return text

def preprocess_dataframe(df: pd.DataFrame, text_column: str = 'message_text', output_column: str = 'preprocessed_text', remove_stopwords: bool = False) -> pd.DataFrame:
    """
    Applies the comprehensive Amharic preprocessing pipeline to a specified text column in a DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.
        text_column (str): The name of the column containing the raw text messages.
        output_column (str): The name of the new column to store the preprocessed text.
        remove_stopwords (bool): Whether to apply stop word removal during preprocessing.

    Returns:
        pd.DataFrame: A new DataFrame with the preprocessed text column added.
    """
    if text_column not in df.columns:
        logger.error(f"DataFrame must contain a '{text_column}' column for preprocessing.")
        return df

    logger.info(f"Starting text preprocessing on column '{text_column}'...")
    
    # Fill NaN values in the text column with empty strings to prevent errors
    df_copy = df.copy()
    df_copy[text_column] = df_copy[text_column].fillna('')

    df_copy[output_column] = df_copy[text_column].apply(lambda x: preprocess_amharic_text(x, remove_stopwords=remove_stopwords))
    
    logger.info("Text preprocessing complete.")
    return df_copy

if __name__ == '__main__':
    # --- Example Usage for individual functions ---
    print("--- Individual Function Tests ---")
    sample_text_complex = "·å§·äì ·ã≠·àµ·å•·àç·äù! ·ãã·åã·ãç ·çª·ç≥·ç≠ ·â•·à≠ ·äê·ãç·ç¢ ·ä†·ãµ·à´·àª·âΩ·äï ·àò·åà·äì·äõ ·àµ·à™ ·ä§·àù ·à≤·â≤ ·àû·àç ·äê·ãç·ç¢ @Shageronlinestore #·âÖ·äì·àΩ üòä https://t.me/example_product"
    print(f"Original: {sample_text_complex}")

    step1 = apply_unicode_normalization(sample_text_complex)
    print(f"1. Unicode Norm: {step1}")

    step2 = replace_amharic_characters(step1)
    print(f"2. Char Replace: {step2}")

    step3 = normalize_amharic_numerals(step2)
    print(f"3. Numeral Norm: {step3}")

    step4 = remove_urls_mentions_hashtags(step3)
    print(f"4. Removed URLs/Mentions/Hashtags: {step4}")

    step5 = remove_emojis_and_non_amharic_non_ascii(step4)
    print(f"5. Removed Emojis/Non-Amharic: {step5}")

    step6 = normalize_punctuation(step5)
    print(f"6. Punctuation Norm: {step6}")

    step7 = remove_extra_whitespace(step6)
    print(f"7. Whitespace Norm: {step7}")

    final_processed = preprocess_amharic_text(sample_text_complex)
    print(f"\nFinal Preprocessed (no stopwords): {final_processed}")

    # --- Example Usage for DataFrame processing ---
    print("\n--- DataFrame Preprocessing Test ---")
    dummy_data = {
        'message_id': [1, 2, 3, 4, 5],
        'message_text': [
            "·å§·äì ·ã≠·àµ·å•·àç·äù! ·ãã·åã·ãç 500 ·â•·à≠ ·äê·ãç·ç¢ ·ä†·ãµ·à´·àª·âΩ·äï ·àò·åà·äì·äõ ·àµ·à™ ·ä§·àù ·à≤·â≤ ·àû·àç ·äê·ãç·ç¢ @Shageronlinestore #·âÖ·äì·àΩ",
            "·ã≠·àÖ ·àù·à≠·âµ ·â†·å£·àù ·âÜ·äï·åÜ ·äê·ãç·ç¢ ·ãã·åã·ç¶ 1,200 ·â•·à≠. https://t.me/example_product",
            "·ä†·ã≤·àµ ·ä•·âÉ ·åà·â•·â∑·àç!!! ·ãç·àµ·äï ·çç·à¨ ·äê·ãç ·ã´·àà·ãç·ç¢",
            "Hello, this is a test message. Some ·ãé·à≠·ã∂·âΩ and numbers like ·ç≥·ç´·ç¨·ç≠.", # Mixed text & Geez numerals
            "·ç© ·ç™ ·ç´ ·ç¨ ·ç≠ ·çÆ ·çØ ·ç∞ ·ç± ·ç≤ ·ç≥ ·çª·ç¢ ·ã®·ãã·åã ·âÖ·äì·àΩ ·ä†·àà·ç¢" # Pure Geez numerals
        ],
        'message_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],
        'views': [100, 150, 80, 120, 90],
        'media_path': [None, 'photos/ch1_msg2.jpg', None, None, None]
    }
    dummy_df = pd.DataFrame(dummy_data)
    
    print("\nOriginal DataFrame Sample:")
    print(dummy_df[['message_id', 'message_text']].head())

    processed_df = preprocess_dataframe(dummy_df.copy())
    print("\nProcessed DataFrame Sample:")
    print(processed_df[['message_id', 'message_text', 'preprocessed_text']].head())

    # Example with stopword removal (assuming AMHARIC_STOP_WORDS is populated)
    # For demonstration, let's temporarily populate it
    AMHARIC_STOP_WORDS.update(['·äê·ãç', '·ã®', '·ä•·äì', '·â†', '·àà', '·ä®'])
    processed_df_with_stopwords = preprocess_dataframe(dummy_df.copy(), remove_stopwords=True)
    print("\nProcessed DataFrame with Stopwords Removed Sample:")
    print(processed_df_with_stopwords[['message_id', 'message_text', 'preprocessed_text']].head())
    
    # Clean up the temporary stop word addition
    AMHARIC_STOP_WORDS.clear()

    # Verify structural integrity (example of saving)
    # Note: Adjust path if running from main project root for testing purposes
    # For actual pipeline execution, run_pipeline.py handles the path
    output_test_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'data', 'processed', 'processed_telegram_data_test.csv')
    os.makedirs(os.path.dirname(output_test_path), exist_ok=True)
    processed_df.to_csv(output_test_path, index=False, encoding='utf-8')
    logger.info(f"Test processed data saved to {output_test_path}")

